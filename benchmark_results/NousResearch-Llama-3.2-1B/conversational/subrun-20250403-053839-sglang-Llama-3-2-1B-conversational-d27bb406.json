{
  "id": "subrun-20250403-053839-sglang-Llama-3-2-1B-conversational-d27bb406",
  "parent_run_id": "run-20250403-053839-sglang-Llama-3-2-1B-conversational-652d19da",
  "engine": {
    "name": "sglang",
    "type": "sglang",
    "model": "NousResearch/Llama-3.2-1B",
    "status": "running",
    "container_id": "402fa93d836c",
    "engine_args": {
      "model-path": "NousResearch/Llama-3.2-1B",
      "host": "0.0.0.0",
      "dtype": "half"
    },
    "env_vars": {},
    "full_engine_args": {
      "model_path": "NousResearch/Llama-3.2-1B",
      "tokenizer_path": "NousResearch/Llama-3.2-1B",
      "tokenizer_mode": "auto",
      "skip_tokenizer_init": false,
      "load_format": "auto",
      "trust_remote_code": false,
      "dtype": "half",
      "kv_cache_dtype": "auto",
      "quantization": null,
      "quantization_param_path": null,
      "context_length": null,
      "device": "cuda",
      "served_model_name": "NousResearch/Llama-3.2-1B",
      "chat_template": null,
      "completion_template": null,
      "is_embedding": false,
      "revision": null,
      "host": "0.0.0.0",
      "port": 30000,
      "mem_fraction_static": 0.88,
      "max_running_requests": null,
      "max_total_tokens": null,
      "chunked_prefill_size": null,
      "max_prefill_tokens": 16384,
      "schedule_policy": "fcfs",
      "schedule_conservativeness": 1.0,
      "cpu_offload_gb": 0,
      "page_size": 1,
      "tp_size": 1,
      "stream_interval": 1,
      "stream_output": false,
      "random_seed": 663796607,
      "constrained_json_whitespace_pattern": null,
      "watchdog_timeout": 300,
      "dist_timeout": null,
      "download_dir": null,
      "base_gpu_id": 0,
      "gpu_id_step": 1,
      "log_level": "info",
      "log_level_http": null,
      "log_requests": false,
      "log_requests_level": 0,
      "show_time_cost": false,
      "enable_metrics": false,
      "decode_log_interval": 40,
      "api_key": null,
      "file_storage_path": "sglang_storage",
      "enable_cache_report": false,
      "reasoning_parser": null,
      "dp_size": 1,
      "load_balance_method": "round_robin",
      "ep_size": 1,
      "dist_init_addr": null,
      "nnodes": 1,
      "node_rank": 0,
      "json_model_override_args": "{}",
      "lora_paths": null,
      "max_loras_per_batch": 8,
      "lora_backend": "triton",
      "attention_backend": "flashinfer",
      "sampling_backend": "flashinfer",
      "grammar_backend": "xgrammar",
      "speculative_algorithm": null,
      "speculative_draft_model_path": null,
      "speculative_num_steps": 5,
      "speculative_eagle_topk": 4,
      "speculative_num_draft_tokens": 8,
      "speculative_accept_threshold_single": 1.0,
      "speculative_accept_threshold_acc": 1.0,
      "speculative_token_map": null,
      "enable_double_sparsity": false,
      "ds_channel_config_path": null,
      "ds_heavy_channel_num": 32,
      "ds_heavy_token_num": 256,
      "ds_heavy_channel_type": "qk",
      "ds_sparse_decode_threshold": 4096,
      "disable_radix_cache": false,
      "disable_cuda_graph": false,
      "disable_cuda_graph_padding": false,
      "enable_nccl_nvls": false,
      "disable_outlines_disk_cache": false,
      "disable_custom_all_reduce": false,
      "disable_mla": false,
      "disable_overlap_schedule": false,
      "enable_mixed_chunk": false,
      "enable_dp_attention": false,
      "enable_ep_moe": false,
      "enable_deepep_moe": false,
      "enable_torch_compile": false,
      "torch_compile_max_bs": 32,
      "cuda_graph_max_bs": 160,
      "cuda_graph_bs": null,
      "torchao_config": "",
      "enable_nan_detection": false,
      "enable_p2p_check": false,
      "triton_attention_reduce_in_fp32": false,
      "triton_attention_num_kv_splits": 8,
      "num_continuous_decode_steps": 1,
      "delete_ckpt_after_loading": false,
      "enable_memory_saver": false,
      "allow_auto_truncate": false,
      "enable_custom_logit_processor": false,
      "tool_call_parser": null,
      "enable_hierarchical_cache": false,
      "hicache_ratio": 2.0,
      "enable_flashinfer_mla": false,
      "enable_flashmla": false,
      "flashinfer_mla_disable_ragged": false,
      "warmups": null,
      "debug_tensor_dump_output_folder": null,
      "debug_tensor_dump_input_file": null,
      "debug_tensor_dump_inject": false,
      "disaggregation_mode": "null",
      "disaggregation_bootstrap_port": 8998,
      "status": "ready",
      "max_total_num_tokens": 1019113,
      "max_req_input_len": 131066,
      "last_gen_throughput": 0.0,
      "version": "0.4.4.post3"
    },
    "full_env_vars": {
      "PATH": "/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin",
      "CUDA_VERSION": "12.4.1.003",
      "CUDA_DRIVER_VERSION": "550.54.15",
      "CUDA_CACHE_DISABLE": "1",
      "NVIDIA_REQUIRE_JETPACK_HOST_MOUNTS": "",
      "_CUDA_COMPAT_PATH": "/usr/local/cuda/compat",
      "ENV": "/etc/shinit_v2",
      "BASH_ENV": "/etc/bash.bashrc",
      "SHELL": "/bin/bash",
      "NVIDIA_REQUIRE_CUDA": "cuda>=9.0",
      "NCCL_VERSION": "2.21.5",
      "CUBLAS_VERSION": "12.4.5.8",
      "CUFFT_VERSION": "11.2.1.3",
      "CURAND_VERSION": "10.3.5.147",
      "CUSPARSE_VERSION": "12.3.1.170",
      "CUSOLVER_VERSION": "11.6.1.9",
      "CUTENSOR_VERSION": "2.0.1.2",
      "NPP_VERSION": "12.2.5.30",
      "NVJPEG_VERSION": "12.3.1.117",
      "CUDNN_VERSION": "9.1.0.70",
      "TRT_VERSION": "8.6.3.1+cuda12.2.2.009",
      "TRTOSS_VERSION": "23.11",
      "NSIGHT_SYSTEMS_VERSION": "2024.2.1.106",
      "NSIGHT_COMPUTE_VERSION": "2024.1.1.4",
      "DALI_VERSION": "1.36.0",
      "DALI_BUILD": "13435171",
      "POLYGRAPHY_VERSION": "0.49.8",
      "TRANSFORMER_ENGINE_VERSION": "1.5",
      "LD_LIBRARY_PATH": "/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64",
      "NVIDIA_VISIBLE_DEVICES": "all",
      "NVIDIA_DRIVER_CAPABILITIES": "compute,utility,video",
      "NVIDIA_PRODUCT_NAME": "Triton Server Base",
      "GDRCOPY_VERSION": "2.3.1-1",
      "HPCX_VERSION": "2.18",
      "MOFED_VERSION": "5.4-rdmacore39.0",
      "OPENUCX_VERSION": "1.16.0",
      "OPENMPI_VERSION": "4.1.7",
      "RDMACORE_VERSION": "39.0",
      "OPAL_PREFIX": "/opt/hpcx/ompi",
      "OMPI_MCA_coll_hcoll_enable": "0",
      "LIBRARY_PATH": "/usr/local/cuda/lib64/stubs:",
      "NVIDIA_TRITON_SERVER_BASE_VERSION": "24.04",
      "NVIDIA_BUILD_ID": "90085237",
      "DEBIAN_FRONTEND": "interactive",
      "LANG": "en_US.UTF-8",
      "LANGUAGE": "en_US:en",
      "LC_ALL": "en_US.UTF-8"
    },
    "version": "0.4.4.post3",
    "endpoint": "http://localhost:30000",
    "gpu_info": {
      "success": true,
      "container_id": "402fa93d836c",
      "container_stats": {
        "cpu_usage": "102.14%",
        "memory_usage": "3.479GiB / 117.9GiB",
        "memory_percent": "2.95%"
      },
      "gpus": [
        {
          "index": 0,
          "name": "NVIDIA A100-SXM4-40GB",
          "memory_total": 39936.0,
          "memory_used": 39438.0,
          "utilization": 0.0
        }
      ]
    }
  },
  "model": "NousResearch/Llama-3.2-1B",
  "benchmark": {
    "type": "conversational",
    "config": {
      "dataset-name": "random",
      "random-input-len": 2000,
      "random-output-len": 100,
      "random-prefix-len": 1000,
      "num-prompts": 50,
      "request-rate": 10
    },
    "command": "python /root/inference-engine-arena/src/benchmarks/benchmark_serving.py --base-url http://localhost:30000 --model NousResearch/Llama-3.2-1B --backend sglang --trust-remote-code --dataset-name random --random-input-len 2000 --random-output-len 100 --random-prefix-len 1000 --num-prompts 50 --request-rate 10 --save-result --result-dir results/run-20250403-053839-sglang-Llama-3-2-1B-conversational-652d19da/subrun-20250403-053839-sglang-Llama-3-2-1B-conversational-d27bb406 --result-filename raw_result.json",
    "namespace_params": {
      "backend": "sglang",
      "base_url": "http://localhost:30000",
      "host": "127.0.0.1",
      "port": 8000,
      "endpoint": "/v1/completions",
      "dataset_name": "random",
      "dataset_path": null,
      "max_concurrency": null,
      "model": "NousResearch/Llama-3.2-1B",
      "tokenizer": null,
      "use_beam_search": false,
      "num_prompts": 50,
      "logprobs": null,
      "request_rate": 10.0,
      "burstiness": 1.0,
      "seed": 0,
      "trust_remote_code": true,
      "disable_tqdm": false,
      "profile": false,
      "save_result": true,
      "save_detailed": false,
      "metadata": null,
      "result_dir": "results/run-20250403-053839-sglang-Llama-3-2-1B-conversational-652d19da/subrun-20250403-053839-sglang-Llama-3-2-1B-conversational-d27bb406",
      "result_filename": "raw_result.json",
      "ignore_eos": false,
      "percentile_metrics": "ttft",
      "metric_percentiles": "99",
      "goodput": null,
      "sonnet_input_len": 550,
      "sonnet_output_len": 150,
      "sonnet_prefix_len": 200,
      "sharegpt_output_len": null,
      "random_input_len": 2000,
      "random_output_len": 100,
      "random_range_ratio": 1.0,
      "random_prefix_len": 1000,
      "hf_subset": null,
      "hf_split": null,
      "hf_output_len": null,
      "tokenizer_mode": "auto",
      "served_model_name": null
    }
  },
  "start_time": "2025-04-03T05:38:39.942671",
  "end_time": "2025-04-03T05:38:56.994111",
  "duration_seconds": 17.05144,
  "success": true,
  "metrics": {
    "input_throughput": 24446.7833297748,
    "output_throughput": 640.6687017956316,
    "ttft": 146.3959365600067,
    "tpot": 8.807542433961835,
    "duration": 6.13577655500012,
    "num_prompts": 50,
    "total_input_tokens": 150000,
    "total_output_tokens": 3931
  },
  "exit_code": 0,
  "stdout": "INFO 04-03 05:38:44 [__init__.py:239] Automatically detected platform cuda.\nNamespace(backend='sglang', base_url='http://localhost:30000', host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='random', dataset_path=None, max_concurrency=None, model='NousResearch/Llama-3.2-1B', tokenizer=None, use_beam_search=False, num_prompts=50, logprobs=None, request_rate=10.0, burstiness=1.0, seed=0, trust_remote_code=True, disable_tqdm=False, profile=False, save_result=True, save_detailed=False, metadata=None, result_dir='results/run-20250403-053839-sglang-Llama-3-2-1B-conversational-652d19da/subrun-20250403-053839-sglang-Llama-3-2-1B-conversational-d27bb406', result_filename='raw_result.json', ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=2000, random_output_len=100, random_range_ratio=1.0, random_prefix_len=1000, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 10.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     50        \nBenchmark duration (s):                  6.14      \nTotal input tokens:                      150000    \nTotal generated tokens:                  3931      \nRequest throughput (req/s):              8.15      \nOutput token throughput (tok/s):         640.67    \nTotal Token throughput (tok/s):          25087.45  \n---------------Time to First Token----------------\nMean TTFT (ms):                          146.40    \nMedian TTFT (ms):                        65.51     \nP99 TTFT (ms):                           837.19    \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          8.81      \nMedian TPOT (ms):                        4.78      \nP99 TPOT (ms):                           68.64     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           6.12      \nMedian ITL (ms):                         3.36      \nP99 ITL (ms):                            46.65     \n==================================================\n",
  "stderr": "\n  0%|          | 0/50 [00:00<?, ?it/s]\n  2%|\u258f         | 1/50 [00:01<01:06,  1.35s/it]\n  8%|\u258a         | 4/50 [00:01<00:17,  2.64it/s]\n 12%|\u2588\u258f        | 6/50 [00:01<00:11,  3.92it/s]\n 32%|\u2588\u2588\u2588\u258f      | 16/50 [00:02<00:02, 14.04it/s]\n 40%|\u2588\u2588\u2588\u2588      | 20/50 [00:02<00:02, 13.53it/s]\n 46%|\u2588\u2588\u2588\u2588\u258c     | 23/50 [00:02<00:02, 12.96it/s]\n 52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 26/50 [00:03<00:02, 10.76it/s]\n 56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 28/50 [00:03<00:02,  9.28it/s]\n 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 30/50 [00:03<00:02,  8.23it/s]\n 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 32/50 [00:04<00:02,  6.82it/s]\n 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 33/50 [00:04<00:02,  5.80it/s]\n 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 35/50 [00:04<00:02,  7.15it/s]\n 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 37/50 [00:04<00:01,  7.23it/s]\n 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 39/50 [00:05<00:01,  6.82it/s]\n 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 41/50 [00:05<00:01,  7.03it/s]\n 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 44/50 [00:05<00:00,  7.22it/s]\n 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 47/50 [00:06<00:00,  9.85it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [00:06<00:00,  8.15it/s]\n"
}