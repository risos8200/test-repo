{
  "id": "subrun-20250403-052601-vllm-Llama-3-2-1B-conversational-e3d74a14",
  "parent_run_id": "run-20250403-052601-vllm-Llama-3-2-1B-conversational-ee354505",
  "engine": {
    "name": "vllm",
    "type": "vllm",
    "model": "NousResearch/Llama-3.2-1B",
    "status": "running",
    "container_id": "9336b1398857",
    "engine_args": {
      "model": "NousResearch/Llama-3.2-1B",
      "trust-remote-code": true,
      "disable-log-requests": true,
      "dtype": "half"
    },
    "env_vars": {
      "VLLM_USE_V1": "1",
      "VLLM_USAGE_SOURCE": "production-docker-image"
    },
    "full_engine_args": {
      "model": "NousResearch/Llama-3.2-1B",
      "speculative_config": null,
      "tokenizer": "NousResearch/Llama-3.2-1B",
      "skip_tokenizer_init": false,
      "tokenizer_mode": "auto",
      "revision": null,
      "override_neuron_config": null,
      "tokenizer_revision": null,
      "trust_remote_code": true,
      "dtype": "torch.float16",
      "max_seq_len": 131072,
      "download_dir": null,
      "load_format": "LoadFormat.AUTO",
      "tensor_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "disable_custom_all_reduce": false,
      "quantization": null,
      "enforce_eager": false,
      "kv_cache_dtype": "auto",
      "device_config": "cuda",
      "decoding_config": {
        "type": "DecodingConfig",
        "params": {
          "guided_decoding_backend": "xgrammar",
          "reasoning_backend": null
        }
      },
      "observability_config": {
        "type": "ObservabilityConfig",
        "params": {
          "show_hidden_metrics": false,
          "otlp_traces_endpoint": null,
          "collect_model_forward_time": false,
          "collect_model_execute_time": false
        }
      },
      "seed": null,
      "served_model_name": "NousResearch/Llama-3.2-1B",
      "num_scheduler_steps": 1,
      "multi_step_stream_outputs": true,
      "enable_prefix_caching": true,
      "chunked_prefill_enabled": true,
      "use_async_output_proc": true,
      "disable_mm_preprocessor_cache": false,
      "mm_processor_kwargs": null,
      "pooler_config": null,
      "compilation_config": {
        "level": 3,
        "custom_ops": [
          "none"
        ],
        "splitting_ops": [
          "vllm.unified_attention",
          "vllm.unified_attention_with_output"
        ],
        "use_inductor": true,
        "compile_sizes": [],
        "use_cudagraph": true,
        "cudagraph_num_of_warmups": 1,
        "cudagraph_capture_sizes": [
          512,
          504,
          496,
          488,
          480,
          472,
          464,
          456,
          448,
          440,
          432,
          424,
          416,
          408,
          400,
          392,
          384,
          376,
          368,
          360,
          352,
          344,
          336,
          328,
          320,
          312,
          304,
          296,
          288,
          280,
          272,
          264,
          256,
          248,
          240,
          232,
          224,
          216,
          208,
          200,
          192,
          184,
          176,
          168,
          160,
          152,
          144,
          136,
          128,
          120,
          112,
          104,
          96,
          88,
          80,
          72,
          64,
          56,
          48,
          40,
          32,
          24,
          16,
          8,
          4,
          2,
          1
        ],
        "max_capture_size": 512
      }
    },
    "full_env_vars": {
      "VLLM_USE_V1": "1",
      "PATH": "/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
      "NVARCH": "x86_64",
      "NVIDIA_REQUIRE_CUDA": "cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536",
      "NV_CUDA_CUDART_VERSION": "12.4.99-1",
      "NV_CUDA_COMPAT_PACKAGE": "cuda-compat-12-4",
      "CUDA_VERSION": "12.4.0",
      "LD_LIBRARY_PATH": "/usr/local/nvidia/lib:/usr/local/nvidia/lib64",
      "NVIDIA_VISIBLE_DEVICES": "all",
      "NVIDIA_DRIVER_CAPABILITIES": "compute,utility",
      "NV_CUDA_LIB_VERSION": "12.4.0-1",
      "NV_NVTX_VERSION": "12.4.99-1",
      "NV_LIBNPP_VERSION": "12.2.5.2-1",
      "NV_LIBNPP_PACKAGE": "libnpp-12-4=12.2.5.2-1",
      "NV_LIBCUSPARSE_VERSION": "12.3.0.142-1",
      "NV_LIBCUBLAS_PACKAGE_NAME": "libcublas-12-4",
      "NV_LIBCUBLAS_VERSION": "12.4.2.65-1",
      "NV_LIBCUBLAS_PACKAGE": "libcublas-12-4=12.4.2.65-1",
      "NV_LIBNCCL_PACKAGE_NAME": "libnccl2",
      "NV_LIBNCCL_PACKAGE_VERSION": "2.20.5-1",
      "NCCL_VERSION": "2.20.5-1",
      "NV_LIBNCCL_PACKAGE": "libnccl2=2.20.5-1+cuda12.4",
      "NVIDIA_PRODUCT_NAME": "CUDA",
      "NV_CUDA_CUDART_DEV_VERSION": "12.4.99-1",
      "NV_NVML_DEV_VERSION": "12.4.99-1",
      "NV_LIBCUSPARSE_DEV_VERSION": "12.3.0.142-1",
      "NV_LIBNPP_DEV_VERSION": "12.2.5.2-1",
      "NV_LIBNPP_DEV_PACKAGE": "libnpp-dev-12-4=12.2.5.2-1",
      "NV_LIBCUBLAS_DEV_VERSION": "12.4.2.65-1",
      "NV_LIBCUBLAS_DEV_PACKAGE_NAME": "libcublas-dev-12-4",
      "NV_LIBCUBLAS_DEV_PACKAGE": "libcublas-dev-12-4=12.4.2.65-1",
      "NV_CUDA_NSIGHT_COMPUTE_VERSION": "12.4.0-1",
      "NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE": "cuda-nsight-compute-12-4=12.4.0-1",
      "NV_NVPROF_VERSION": "12.4.99-1",
      "NV_NVPROF_DEV_PACKAGE": "cuda-nvprof-12-4=12.4.99-1",
      "NV_LIBNCCL_DEV_PACKAGE_NAME": "libnccl-dev",
      "NV_LIBNCCL_DEV_PACKAGE_VERSION": "2.20.5-1",
      "NV_LIBNCCL_DEV_PACKAGE": "libnccl-dev=2.20.5-1+cuda12.4",
      "LIBRARY_PATH": "/usr/local/cuda/lib64/stubs",
      "DEBIAN_FRONTEND": "noninteractive",
      "UV_HTTP_TIMEOUT": "500",
      "VLLM_USAGE_SOURCE": "production-docker-image"
    },
    "version": "0.8.2",
    "endpoint": "http://localhost:8000",
    "gpu_info": {
      "success": true,
      "container_id": "9336b1398857",
      "container_stats": {
        "cpu_usage": "1.65%",
        "memory_usage": "2.622GiB / 117.9GiB",
        "memory_percent": "2.22%"
      },
      "gpus": [
        {
          "index": 0,
          "name": "NVIDIA A100-SXM4-40GB",
          "memory_total": 39936.0,
          "memory_used": 35873.0,
          "utilization": 0.0
        }
      ]
    }
  },
  "model": "NousResearch/Llama-3.2-1B",
  "benchmark": {
    "type": "conversational",
    "config": {
      "dataset-name": "random",
      "random-input-len": 2000,
      "random-output-len": 100,
      "random-prefix-len": 1000,
      "num-prompts": 50,
      "request-rate": 10
    },
    "command": "python /root/inference-engine-arena/src/benchmarks/benchmark_serving.py --base-url http://localhost:8000 --model NousResearch/Llama-3.2-1B --backend vllm --trust-remote-code --dataset-name random --random-input-len 2000 --random-output-len 100 --random-prefix-len 1000 --num-prompts 50 --request-rate 10 --save-result --result-dir results/run-20250403-052601-vllm-Llama-3-2-1B-conversational-ee354505/subrun-20250403-052601-vllm-Llama-3-2-1B-conversational-e3d74a14 --result-filename raw_result.json",
    "namespace_params": {}
  },
  "start_time": "2025-04-03T05:26:01.451875",
  "end_time": "2025-04-03T05:26:09.669717",
  "duration_seconds": 8.217842,
  "success": false,
  "metrics": {},
  "exit_code": 1,
  "stdout": "INFO 04-03 05:26:05 [__init__.py:239] Automatically detected platform cuda.\n",
  "stderr": "Traceback (most recent call last):\n  File \"/root/inference-engine-arena/src/benchmarks/benchmark_serving.py\", line 52, in <module>\n    from benchmark_dataset import (BurstGPTDataset, HuggingFaceDataset,\n  File \"/root/inference-engine-arena/src/benchmarks/benchmark_dataset.py\", line 29, in <module>\n    import pandas as pd\nModuleNotFoundError: No module named 'pandas'\n"
}